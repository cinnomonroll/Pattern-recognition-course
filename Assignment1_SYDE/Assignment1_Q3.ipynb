{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will use the same data as in exercise 1 with 2×1 vectors, for the ML and MAP classifiers.\n",
    "\n",
    "\n",
    "Q3-1 \n",
    "\n",
    "Find the sample mean and covariance for the training set of the two classes in the MNIST dataset\n",
    "and estimate the probability of the two classes as Gaussian distributions. Based on this, develop an ML\n",
    "classifier and report the classification accuracy on the test set of the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset, False makes it return the data as a NumPy array \n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False, parser='liac-arff')\n",
    "\n",
    "# Flatten the images\n",
    "X = mnist.data\n",
    "y = mnist.target\n",
    "#print(X.shape) #(70000, 784)\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, y_train = X[:60000], y[:60000]\n",
    "X_test, y_test = X[60000:], y[60000:]\n",
    "\n",
    "# Filter the training data for classes 3 and 4\n",
    "mask_train = np.isin(y_train, ['3', '4'])\n",
    "X_train, y_train = X_train[mask_train], y_train[mask_train]\n",
    "mask_test = np.isin(y_test, ['3', '4'])\n",
    "X_test, y_test = X_test[mask_test], y_test[mask_test]\n",
    "\n",
    "# Convert labels to integers\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PCA to convert the 11973x784 vectors to 11973x2 vectors\n",
    "pca_train = PCA(n_components=2)\n",
    "X_pca_train = pca_train.fit_transform(X_train) #print(X_pca.shape) = (11973, 2)\n",
    "pca_test = PCA(n_components=2)\n",
    "X_pca_test = pca_test.fit_transform(X_test) #print(X_pca.shape) = (1992, 2)\n",
    "\n",
    "# print(X_pca_train.shape) #(11973, 2)\n",
    "# print(y_train.shape) #(11973,)\n",
    "# print(X_pca_test.shape) #(1992, 2)\n",
    "# print(y_test.shape) #(1992, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the training data\n",
    "X_train_3 = X_pca_train[y_train == 3] #(6131, 2)\n",
    "X_train_4 = X_pca_train[y_train == 4]\n",
    "\n",
    "# Compute the mean \n",
    "mean_train_3 = np.mean(X_train_3, axis=0) #(2,)\n",
    "mean_train_4 = np.mean(X_train_4, axis=0)\n",
    "\n",
    "# Compute the covariance\n",
    "cov_train_3 = np.cov(X_train_3, rowvar=False) #(2,2)\n",
    "cov_train_4 = np.cov(X_train_4, rowvar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML classification accuracy: 0.981425702811245\n"
     ]
    }
   ],
   "source": [
    "def matrix_inverse(matrix):\n",
    "    # Calculate the determinant of the matrix\n",
    "    determinant = np.linalg.det(matrix)\n",
    "    \n",
    "    # Check if the matrix is invertible\n",
    "    if determinant == 0:\n",
    "        raise ValueError(\"The matrix is not invertible.\")\n",
    "    \n",
    "    # Calculate the adjugate of the matrix\n",
    "    adjugate = np.zeros(matrix.shape)\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            minor = np.delete(np.delete(matrix, i, axis=0), j, axis=1)\n",
    "            adjugate[j, i] = ((-1) ** (i + j)) * np.linalg.det(minor)\n",
    "    \n",
    "    # Calculate the inverse of the matrix\n",
    "    inverse_matrix = adjugate / determinant\n",
    "    \n",
    "    return inverse_matrix\n",
    "\n",
    "# Compute the inverse of the covariance matrices\n",
    "inv_cov_train_3 = matrix_inverse(cov_train_3)\n",
    "inv_cov_train_4 = matrix_inverse(cov_train_4)\n",
    "\n",
    "# Function to compute the log of the Gaussian pdf\n",
    "def log_gaussian_pdf(x, mean, inv_cov):\n",
    "    diff = x - mean\n",
    "    return -0.5 * np.dot(diff, np.dot(inv_cov, diff))\n",
    "\n",
    "# Function to classify a sample\n",
    "def classify(sample):\n",
    "    # Compute the log-likelihood under each class model\n",
    "    log_likelihood_3 = log_gaussian_pdf(sample, mean_train_3 , inv_cov_train_3)\n",
    "    log_likelihood_4 = log_gaussian_pdf(sample, mean_train_4 , inv_cov_train_4)\n",
    "    \n",
    "    # Return the class that gives the highest log-likelihood\n",
    "    return 3 if log_likelihood_3 > log_likelihood_4 else 4\n",
    "\n",
    "# Classify the test samples\n",
    "y_pred_test = np.array([classify(x) for x in X_pca_test])\n",
    "\n",
    "# Compute the classification accuracy\n",
    "accuracy = np.mean(y_pred_test == y_test)\n",
    "print(\"ML classification accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3-2\n",
    "\n",
    "Now, let’s assume that the prior probabilities for the two classes are p(C1) = 0.58 and p(C2) = 0.42.\n",
    "Using these prior probabilities, and the means and covariances of the two classes, develop an MAP\n",
    "classifier and report the classification accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP classification accuracy: 0.9774096385542169\n"
     ]
    }
   ],
   "source": [
    "# Prior probabilities for the two classes\n",
    "prior_3 = 0.58\n",
    "prior_4 = 0.42\n",
    "\n",
    "# Function to classify a sample\n",
    "def classify(sample):\n",
    "    # Compute the log-likelihood under each class model\n",
    "    log_likelihood_3 = log_gaussian_pdf(sample, mean_train_3, inv_cov_train_3) + np.log(prior_3)\n",
    "    log_likelihood_4 = log_gaussian_pdf(sample, mean_train_4, inv_cov_train_4) + np.log(prior_4)\n",
    "    \n",
    "    # Return the class that gives the highest log-likelihood\n",
    "    return 3 if log_likelihood_3 > log_likelihood_4 else 4\n",
    "\n",
    "# Classify the test samples\n",
    "y_pred_test = np.array([classify(x) for x in X_pca_test])\n",
    "\n",
    "# Compute the classification accuracy\n",
    "accuracy = np.mean(y_pred_test == y_test)\n",
    "print(\"MAP classification accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3-3\n",
    "\n",
    "Based on the results, do you think that assuming the probability distributions of the two classes\n",
    "as Gaussian was correct? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both MAP and ML classifications, I’ve used the Gaussian probability density function (pdf) as a modeling choice. The high accuracy achieved by both classifiers suggests that the features of classes 3 and 4 may indeed follow a Gaussian distribution. However, further statistical tests would be needed for confirmation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3-4\n",
    "\n",
    "Compare the ML, MAP, MED, MMD, and kNN classifiers based on the classification accuracy.\n",
    "Which classifier is the best? Could the inferior classifiers be better for different datasets? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML Accuracy: 0.9814 \n",
    "MAP Accuracy: 0.9774 \n",
    "MED Accuracy: 0.9764\n",
    "MMD Accuracy: 0.9819\n",
    "kNN (k=1): 0.9694\n",
    "kNN (k=2): 0.9649\n",
    "kNN (k=3): 0.9769\n",
    "kNN (k=4): 0.9739\n",
    "kNN (k=5): 0.9769\n",
    "\n",
    "When it comes to high accuracy, the MMD and ML classifiers outperform others and their accuracy is very close. The MMD classifier has a slightly higher accuracy than the ML classifier, making it the best classifier for this particular case. However, it's important to note that a classifier that performs well on one dataset may not perform as well on another. For instance, the kNN classifier may work better on a dataset with complex decision boundaries, while the ML or MAP classifiers would be more suitable for a dataset where the class distributions are known and follow the assumed distribution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
